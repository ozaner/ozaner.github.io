\documentclass{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage[margin=1in]{geometry}
\usepackage{titling}

\DeclareMathOperator{\Var}{Var}

\setlength{\droptitle}{-7em}   % This is your set screw

\begin{document}

\title{Math Statistics\\ Semiweekly HW 10}
\author{Ozaner Hansha}
\date{October 20, 2020}
\maketitle

\subsection*{Question 1}
\noindent\textbf{Problem:} Show that if $\hat \theta$ is an asymptotically unbiased \& asymptotically unvarianced (i.e. $\displaystyle\lim_{n\to\infty}\Var(\hat{\theta})=0$) estimator for $\theta$, then it is consistent.
\bigskip

\noindent\textbf{Solution:} First let us establish the following inequalities for any $\epsilon\in\mathbb R^+$, and any $n\in\mathbb N$:
$$0\le P(|\hat\theta_n-\theta|\ge\epsilon)\le\frac{E[|\hat\theta_n-\theta|]}{\epsilon}$$

The LHS being the result of the nonnegativity of probabilities, and the RHS being an application of Markov's inequality. Now note that since the equality above holds for any $n\in\mathbb N$, we can take the limit of each term w.r.t to $n$:
\begin{align*}
\lim_{n\to\infty}0\le\lim_{n\to\infty}P(|\hat\theta_n-\theta|\ge\epsilon)&\le\lim_{n\to\infty}\frac{E[|\hat\theta_n-\theta|]}{\epsilon}\\
0\le\lim_{n\to\infty}P(|\hat\theta_n-\theta|\ge\epsilon)&\le\lim_{n\to\infty}\frac{E[|\hat\theta_n-\theta|]}{\epsilon}\tag{limit of a constant}\\
&=\lim_{n\to\infty}\frac{E\left[\sqrt{(\hat\theta_n-\theta)^2}\right]}{\epsilon}\tag{$|x|=\sqrt{x^2}$}\\
&\le\lim_{n\to\infty}\frac{\sqrt{E\left[(\hat\theta_n-\theta)^2\right]}}{\epsilon}\tag{Jenson's inequality for a concave function}\\
&=\frac{\sqrt{\displaystyle\lim_{n\to\infty}E\left[(\hat\theta_n-\theta)^2\right]}}{\epsilon}\tag{linearity \& powers of limits}\\
&=\frac{\sqrt{\displaystyle\lim_{n\to\infty}\left(\text{Bias}(\hat\theta_n)^2+\Var(\hat\theta_n)\right)}}{\epsilon}\tag{bias-variance decomposition}\\
&=\frac{\sqrt{\displaystyle\lim_{n\to\infty}\text{Bias}(\hat\theta_n)^2+\displaystyle\lim_{n\to\infty}\Var(\hat\theta_n)}}{\epsilon}\tag{linearity of limits}\\
&=\frac{\sqrt{\displaystyle\left(\lim_{n\to\infty}\text{Bias}(\hat\theta_n)\right)^2+\displaystyle\lim_{n\to\infty}\Var(\hat\theta_n)}}{\epsilon}\tag{powers of limits}\\
&=\frac{\sqrt{0^2+0}}{\epsilon}\tag{asymptotically unbiased \& unvarianced}\\
&=0
\end{align*}

And so, put in a cleaner form, we have:
$$0\le\lim_{n\to\infty}P(|\hat\theta_n-\theta|\ge\epsilon)\le0$$

And so, by the squeeze theorem, we have that $\hat\theta_n$ satisfies:
$$\lim_{n\to\infty}P(|\hat\theta_n-\theta|\ge\epsilon)=0$$

Which is precisely the definition of a (weakly) consistent estimator. $\blacksquare$
\end{document}