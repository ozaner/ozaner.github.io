\documentclass{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}

\newcommand{\R}{\mathbb R}
\newcommand{\mbf}[1]{\mathbf #1}

\begin{document}

\title{Honors Calculus III Challenge Problems \#3}
\author{Ozaner Hansha}
\date{November 12, 2018}
\maketitle

\section*{Exercise 1}
\textbf{Solution:} The problem sheet introduces $Q$ like so:
\begin{align*}
  \mbf v_i=\sum_{j=1}^rR^j_i\mbf u_j=\sum_{j=1}^rR^j_i(\sum_{k=1}^mQ^k_j\mbf f_k)\\
\end{align*}

This means that for every $j$ up to $r$:
\begin{align*}
  \mbf u_j=\sum_{k=1}^mQ^k_j\mbf f_k\\
\end{align*}

And so the $r$ columns of $Q$ are just $m$-vectors that are equal to the vectors produced by the Gram-Schmidt process which are, by definition, orthonormal.

\section*{Exercise 2}
\textbf{Solution:} We showed above that the matrix $Q$ is orthogonal (i.e. all it's columns are orthonormal vectors), and so equivalently:
$$Q^\top Q=I$$

Where $I$ is the $m$ dimensional identity matrix. This should be clear as matrix multiplication is a series of dot products of the rows of $Q$ and the columns of $Q^\top$ (or vice versa). Transposing the matrix means this dot product will be between 2 identical \textit{orthogonal} vectors when $i=j$ (equaling 1) but between two non-identical orthogonal vectors when $i\not=j$ (i.e. 0). Thus the matrix will have only 1's on the main diagonal and nowhere else (i.e. the identity matrix).

Recall that the dot product of two vectors can also be written in terms of matrix multiplication (by considering $\mbf a$ as a column matrix):
$$\mbf a\cdot\mbf b=\mbf a^\top\mbf b$$

We can now prove our initial proposition:
\begin{align*}
  Q\mbf x\cdot Q\mbf y&=(Q\mbf x)^\top(Q\mbf y)\tag{dot product is matrix mult.}\\
  &=(\mbf x^\top Q^\top)(Q\mbf y)\tag{anti-distributivity of transpose}\\
  &=\mbf x^\top (Q^\top Q)\mbf y\tag{associativity of matrix mult.}\\
  &=\mbf x^\top I\mbf y\tag{def. of orthogonal matrix}\\
  &=\mbf x^\top\mbf y\tag{def. identity matrix}\\
  &=\mbf x\cdot\mbf y\tag{dot product is matrix mult.}\\
\end{align*}

And so indeed, all orthogonal matrices respect the dot product.

\section*{Exercise 3}
\textbf{Solution:} Remember that $\operatorname{ker}(Q)=\{0\}$ is equivalent to all $\mbf a$ such that:
$$a_1\mbf Q_1+a_2\mbf Q_2+\cdots+a_r\mbf Q_r=0$$

\textit{Where $\mbf Q_j$ denotes the $j$th column of $Q$.}
\\

However, recall from Exercise 1 that $\mbf Q_j=\mbf u_j$ and so for there to be no non-trivial solution to the homogenous equation the following must be true:
$$a_1\mbf u_1+a_2\mbf u_2+\cdots+a_r\mbf u_r=0$$

Since the vectors $\mbf u_j$ were created via GS orthonormalization, they form an orthonormal basis of $\R$ and so are linearly independent. This necessarily implies that there are no non-trivial solutions thus, $\operatorname{ker}(Q)=\{\mbf 0\}$.

\section*{Exercise 4}
\textbf{Solution:} Recall that $A=QR$ which means $A\mbf x=QR\mbf x$. Note that all linear transformations (whose kernel just contains $\mbf 0$) have a unique mapping from $\mbf 0$ to itself. Because of this, for $Q$ to map $R\mbf x$ to $\mbf 0$ means that $R\mbf x=\mbf 0$. And so if $A\mbf x=\mbf 0$ then it must be the case that $R\mbf x=\mbf 0$ due to $Q$ mapping it as such.

Proving it the other way around is as simple as noting that all linear transformations map $\mbf 0$ to itself (uniquely or not) and so if $R\mbf x=0$ then so does $Q$ and thus $A$ since $A=QR$

And so the kernals of $A$ and $R$ are equivalent.
\section*{Exercise 5}
\textbf{Solution:} First we need to apply the GS process to the columns $\mbf a_i$ of $A$, giving us:
$$\mbf u_1=\frac{1}{3}(1,2,2)\ \ \ \  \mbf u_2=\frac{1}{3}(2,-2,1)\ \ \ \ \mbf u_1=\frac{1}{3}(2,1,-2)$$

Putting these into a matrix we get:
$$Q=\frac{1}{3}\begin{bmatrix}
    1 & 2 & 2\\
    2 & -2 & 1\\
    2 & 1 & -2
\end{bmatrix}$$

Now we just need to express the columns of $A$ in terms of this new basis:
\begin{align*}
  \mbf r_1&=(\mbf v_1\cdot \mbf u_1)\mbf e_1+(\mbf v_1\cdot \mbf u_2)\mbf e_2+(\mbf v_1\cdot \mbf u_3)\mbf e_3\\
  \mbf r_2&=(\mbf v_2\cdot \mbf u_1)\mbf e_1+(\mbf v_2\cdot \mbf u_2)\mbf e_2+(\mbf v_2\cdot \mbf u_3)\mbf e_3\\
  \mbf r_3&=(\mbf v_3\cdot \mbf u_1)\mbf e_1+(\mbf v_3\cdot \mbf u_2)\mbf u_2+(\mbf v_3\cdot \mbf u_3)\mbf e_3
\end{align*}

Evaluating these vectors this and organizing them into a matrix, we are left with:
$$R=\begin{bmatrix}
    3 & 0 & 0\\
    0 & 3 & 0\\
    0 & 0 & 3
\end{bmatrix}$$

Since the columns of $R$ are linearly independent, $\operatorname{ker}(R)=\{\mbf 0\}$ which in turn implies $\operatorname{ker}(A)=\{\mbf 0\}$.

\section*{Exercise 6}
\textbf{Solution:} If for every $\mbf u_i$ there was an $\mbf x_i$ that $R$ maps from, we could simply note the following:
\begin{align*}
  &R(a_1\mbf x_1+\cdots+a_r\mbf x_r)=\mbf b\\
  &a_1R\mbf x_1+\cdots+a_rR\mbf x_r=\mbf b\tag{Linearity of $R$}\\
  &a_1\mbf u_1+\cdots+a_r\mbf u_r=\mbf b\tag{def. of $\mbf x_i$}
\end{align*}

And since all vectors in $\R^r$ are expressible as a linear combination of the orthonormal basis $\mbf u_i$, $\mbf b$ must be as well.

\section*{Exercise 7}
\textbf{Solution:} Now remember the span of $R$ is just the set of all linear combinations of its column vectors. But recall that the $i$th column of $R$ was some scalar multiple of the canonical basis vector $\mbf e_i$ (and some possibly 0 multiple of all the previous $\mbf e_{<i}$). Since there were $r$ columns, the span of $R$ must be $\R^r$. This means there must be some solution to $R\mbf x_i=\mbf u_i$ because $\mbf u_i\in\R^r$.

\section*{Exercise 8}
\textbf{Solution:} Just let $\mbf y:=R\mbf x$. And so if there exists an $\mbf x$ such that $A\mbf x=\mbf b$ then:
$$A\mbf x=QR\mbf x=Q\mbf y=\mbf b$$

\section*{Exercise 9}
\textbf{Solution:} Recall that the $\operatorname{ker}(Q)=\{0\}$, meaning that there are no non-trivial solutions to the homogenous equation and thus every non-homogenous equation (i.e. anything of the form $Q\mbf x=\mbf y$) has a unique solution. And so of course, we can solve for it by doing the following:
\begin{align*}
  Q\mbf y=\mbf b\\
  Q^\top Q\mbf y=Q^\top\mbf b\\
  \mbf y=Q^\top\mbf b
\end{align*}

And so $\mbf y$ is uniquely determined by the vector $\mbf b$, and the transpose of $Q$ which is just the vectors $\mbf u_i$ arranged by rows.

\section*{Exercise 10}
\textbf{Solution:} Recall that $\mbf y=Q^\top\mbf b$, this means that:
\begin{align*}
  \mbf y_i&=(Q^\top)^i\cdot\mbf b\\
  &=Q_i\cdot\mbf b\\
  &=\mbf u_i\cdot\mbf b
\end{align*}

Where $(Q^\top)^i$ is the $i$th row of $Q^\top$ and $Q_i$ is the $i$th column of $Q$.

\section*{Exercise 11}
\textbf{Solution:} Remember that in Exercise 7, we established that the $i$th column of $R$ is equal to some scalar times $\mbf e_i$ (plus some possibly 0 multiples of the previous canonical bases). And since $R$ has $r$ columns, that $r$th column too must be nonzero multiple of $\mbf e_r$ (and potentially plus some orthogonal components). And so the last row of $R$ must at least have some nonzero scalar at the $r$th (final) column.

The case where all the $\mbf v_i=\mbf 0$ is the case where the matrix $A$ that we decomposed didn't exist at all and so our reasoning is vacuously true.

\section*{Exercise 12}
\textbf{Solution:} Recall that $R$ spans all of $\R^r$. If the last row was all zeros then there would be no column vector in $R$ that had a component in the $\mbf e_r$ direction. This directly contradicts the fact that $R$ spans all of $\R^r$ and thus the last row can't all be zeros.

\section*{Exercise 13}
\textbf{Solution:} We can turn the matrix into a system of equations:
\begin{align*}
  x_1+2x_2+3x_3+4x_4+5x_5=0\\
  3x_3+6x_4-15x_5=9\\
  -2x_4-6x_5=4
\end{align*}

Solving this out via backsubtituing the last row into the upper ones, we get the following parameterization:
$$\mbf x(t,s)=t(-2,1,0,0,0)+s(-26,0,11,,-3,1)+(-13,0,7,-2,0)$$

\section*{Exercise 14}
\textbf{Solution:} First we perform the GS process on the columns of $A$:
\begin{align*}
  \mbf u_1=\frac{1}{5}(4,2,2,-1)\\
  \mbf u_2=\frac{1}{5}(1,-2,2,4)
\end{align*}

This gives us the matrix:
$$Q=\frac{1}{5}\begin{bmatrix}
    4 & 1 & 0\\
    2 & -2 & 0\\
    2 & 2 & 0\\
    -1 & 4 & 0
\end{bmatrix}$$

Now we just need to express the columns of $A$ in terms of this new basis:
\begin{align*}
  \mbf r_1&=(\mbf a_1\cdot \mbf u_1)\mbf e_1\\
  \mbf r_2&=(\mbf a_2\cdot \mbf u_1)\mbf e_1+(\mbf a_2\cdot \mbf u_2)\mbf e_2\\
  \mbf r_3&=(\mbf a_3\cdot \mbf u_1)\mbf e_1+(\mbf a_3\cdot \mbf u_2)\mbf e_2+(\mbf a_3\cdot \mbf u_3)\mbf e_3\\
  &=(\mbf a_3\cdot \mbf u_1)\mbf e_1+(\mbf a_3\cdot \mbf u_2)\mbf e_2
  % \mbf r_4&=(\mbf a_4\cdot \mbf u_1)\mbf e_1+(\mbf a_4\cdot \mbf u_2)\mbf u_2+(\mbf a_4\cdot \mbf u_3)\mbf e_3+(\mbf a_4\cdot \mbf u_4)\mbf e_4\\
  % &=(\mbf a_4\cdot \mbf u_1)\mbf e_1+(\mbf a_4\cdot \mbf u_2)\mbf u_2
\end{align*}

\textit{Note that $\mbf u_3$ is $\mbf 0$.}\\

Evaluating these vectors and organizing them into a matrix, we are left with:
$$R=\begin{bmatrix}
    5 & 5 & 5\\
    0 & 10 & -5\\
    0 & 0 & 0\\
\end{bmatrix}$$

We can finally solve for the following:
$$R\mbf x=Q^\top\mbf b$$

First we compute $Q^\top\mbf b$:
$$Q^\top\mbf b=(15,-5,0)$$

Now we can write down the system of equations:
\begin{align*}
  5x_1+5x_2+5x_3&=15\\
  10x_2-5x_3&=-5\\
  0&=0
\end{align*}

We can now parameterize this via back substitution to find:
$$\mbf x(t)=\frac{1}{5}\left(t,\frac{10-t}{3},\frac{20-2t}{3}\right)$$

\end{document}
