## MC Q's
1. Substrate Chauvinism is the view that:
    B.  Some physical substrates are superior to others.
2. Picard's defense of Data as a person hinges on the fact that:
    D.  We have no way to distinguish between genuine and mimicked personhood.
3. Singer argues the boundary of the ethical community should be drawn based on suffering, because:
    A.  The capacity to suffer is closely tied to having interests
4. The major problem with what we've termed "internal state accounts of personhood" is:
    C.  Internal states can't be verified through external observation
5. If an AI passes the Turing test, it proves:
    C.  The AI can mimic human behavior well in a limited context.
6. "Your argument only applies to current technology. Quantum computing will make AI consciousness possible" is a version of which objection to Searle's Chinese Room:
    A.  The Many Mansions Reply
7. According to Hutchins, the key feature of the speed bugs in a cockpit
    A.  They reduce cognitive load by changing the nature of the pilot's tasks.
8. According to McCarthy:
    C.  We should never try to give AI anything comparable to emotions.
9. According to Sloman, the nature of consciousness as a "cluster concept" is especially problematic because:
    C.  The conceptual disagreements make it especially difficult to settle on what evidence could solve empirical disagreements
10. What is the major difference between restricted virtual machine functionalism and unrestricted virtual machine functionalism?
    A.  Restricted VMF requires that every sub-state be causally connected in some way to inputs and outputs.

## Short Answer
11. Dennett presents three stances for understanding how systems function. Explain each of these stances and give an account of what it would look like to apply each of them to a Chess AI. Then, explain Dennett's criteria for personhood based on this stance theory, and present one objection to Dennett's conclusions about personhood and how he would likely reply to that objection. Do you find his theory convincing or not? Explain why.

The three stances Dennett put forward in considering the behavior of a system are the physical, design, and intentional stance. The physical stance is the least abstract of the three and is the way we reason about natural non-living physical matter: using physical laws to predict what the matter will do. An example is using the law of gravity to predict what will happen to a  dropped rock.

The design stance is taken to reason about objects designed by something for some purpose. Such systems are better understood in terms of trying to achieve that purpose rather than by the explicit physical systems that implement it (at least most of the time). An example of this is trying to predict and change the behavior of an alarm clock. Knowing that it goes off at some set tie in the future and that you can change this time is sufficient to use it. There's no need to consider the underlying clock mechanics, or electronics for phone alarms, that implement this system.

The intentional stance is taken to reason about systems that have interests, or at least appear to, beliefs and the capacity to do things to further those interests. An example of this is knowing that a hungry animal will seek out food, or that a human that's angry at you may ignore or be antagonistic towards you.

In terms of a chess AI, the physical stance would seek to explain its behavior by detailing its physical construction, and the electronic reactions occurring in the computer it takes place in. The design stance would study its programming, giving us insight into how it makes moves but only after pouring through many lines of code and complicated sub-systems. The intentional stance would treat the chess AI as a real agent and not just a program computing moves. Here, understanding its behavior comes down to what you think its goals are (e.g. beating you at chess, trying to trick you with this setup, etc.) and what its means of achieving the things it wants are (being able to compute moves farther ahead than you). You can know the chess AI will make the best move from this stance (assuming its decent) regardless of its implementation or the other low level stuff.

Because you can 'usefully and voluminously' predict the behavior of a chess AI from the intentional stance, it is an intentional system.

Dennett notes that there is no way to distinguish the 'real intentionality' that we may ascribe to a person from the 'derived intentionality' we may ascribe a computer program. This combined with the fact that there is a continuum of degrees of intentionality a system can have leads to Dennett's criteria for personhood being a sufficiently advanced intentional system. That being a person is just an intentional system, i.e. it is 'usefully and voluminously' predictable via the intentional stance. Different conditions can be imposed on this criterion, like that the intentional systems be second order (can form beliefs about beliefs).

An objection might be the Martian marionette, that is a body that passed all our tests of being an intentional system but, once opened up, was found to actually be a husk being remotely controlled by a Martian computer via radio waves. The program was simply programed with a finite number of conditional behaviors that mimic what a human would do under similar circumstances. Certainly this couldn't be considered a person?

Dennett would point out that this doesn't disprove anything as this system is the same as any other person, except that its brain is not local to its body. The person is just the entire intentional system of body and brain. And even if the Martian program is controlling multiple husks, the intentional system theory still makes sense. Only in this case the program itself is the intentional system whose actions we predict and explain. As long as we identify which system is autonomous we can find out who the 'person' is, just as my arm isn't a person but I am.

Interpreting his theory as "there couldn't possibly anything that distinguishes a sufficient intentional system from a human or person" then I certainly sympathize with it. Dennett's description of persons as intentional systems tackles those two key facts of not being able to distinguish so called 'true' intentional systems or people from mimicry, and the apparent continuum of intentional systems that can never mesh well with some arbitrary criterion for personhood. That said, I would imagine many would be unsatisfied with the fact that it doesn't address things like 'feeling' and 'what it is to be someone', things that many would consider are the real marks of personhood and not just the behaviors of systems. Although this leads us right back to fact 1 that his theory deals with: the impossibility of distinguishing these so called true systems from fakes. If there is no way to prove a system is a person beyond asserting it, and the only reason we believe in this distinction in the first place is our own cobbled intuition, what are we even talking about?