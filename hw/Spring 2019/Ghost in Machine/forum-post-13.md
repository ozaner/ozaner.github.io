## Forum Post 13
Is it ethical to produce killer robots?

I don't think there are any objective moral facts when it comes to killer robots (or anything for that matter), and if there were any I certainly couldn't tell you them. But I can imagine that many people would feel apprehensive and even 'bad' (in the 'moral' sense) about the production and use of AI based weapons systems with the authority to kill.

Putting aside our initial gut feelings of the 'badness' associated with these killer robots, if people had to assign a reason to dislike of these systems it boils down to 1) the ease of war leading to more war, and 2) the psychological effect such systems would have on the citizenry of its combatants.

1) if AI weapons systems could make the human aspect of war obsolete (or at least the part in mortal danger) then this removes a large burden on whatever country is using them. They no longer need to sacrifice their own troops to achieve the goal of war. One can imagine this might lead to smaller and smaller reasons for instigating war since the human penalty is low for the country using the robots.

One might object to this stating that, in a world of nuclear weapons and cyberterrorism, so called 'hot' wars are still too costly for any side. Although this may not apply to smaller countries, most smaller counties have some bigger country backing them up. Moreover warmongering on any scale would be penalized by those big countries.

2) The psychological effect of killer robots invading and capturing civilians homeland is something we may not like to think about. We instead may want to sacrifice our won soldiers lives as to avoid this.

One may object that war is psychologically damning enough. Essentially sacrificing soldiers so that the opposing civilians feel better seems ridiculous when phrased this way.

Ultimately it would seem that, unless whatever incarnation of these AI weapon systems pose major power/political differences between those who have them and don't (and indeed sufficiently advanced ones probably will), the objection people have to robot killing systems is that they will either be misused in the real world, or that they seem scary to their cobbled 'moral' sensibilities. Human killing ok, Robot killing not.

The objection that robots may malfunction is not a decent one because whatever the rate of robot malfunction is, human malfunction is higher. "But what if a robot kills someone by accident? Whos responsible?" This is a 'scary' objection based on our feeling that punishment is deserved when something we don't like happens, not a product of robots' inability to perform their task (take down the enemy) quicker and with less casualties.

The objection that the robots will be to easy to be misused is a stronger one. Assuming unnecessary killing is 'bad' (and I'm guessing it is), robots being misused by their commanders (e.g. "just kill everybody its quicker") is bound to happen unless sufficient oversight is implemented. But this produces inefficiencies that may not be tolerated in the fast paced and secretive environment of war/national security.